#!/usr/bin/env python
# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2015 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import os
import sys
import time
import logging
import argparse
import signal
import multiprocessing
import traceback
import datetime

sys.path.insert(0, os.path.join(os.path.abspath(os.path.dirname(__file__)), ".."))

from lib.cuckoo.common.config import Config
from lib.cuckoo.common.constants import CUCKOO_ROOT
from lib.cuckoo.core.database import Database, TASK_REPORTED, TASK_COMPLETED
from lib.cuckoo.core.database import Task, TASK_FAILED_PROCESSING
from lib.cuckoo.core.plugins import RunProcessing, RunSignatures, RunReporting
from lib.cuckoo.core.startup import init_modules, drop_privileges

log = None

# We keep a reporting queue with at most a few hundred entries.
QUEUE_THRESHOLD = 128

import code, traceback, signal

def debug(sig, frame):
    """Interrupt running process, and provide a python prompt for
    interactive debugging."""
    d={'_frame':frame}         # Allow access to frame object.
    d.update(frame.f_globals)  # Unless shadowed by global
    d.update(frame.f_locals)

    i = code.InteractiveConsole(d)
    message  = "Signal received : entering python shell.\nTraceback:\n"
    message += ''.join(traceback.format_stack(frame))
    i.interact(message)

def listen():
    signal.signal(signal.SIGUSR1, debug)  # Register handler

def process(target=None, copy_path=None, task=None, report=False, auto=False):
    log.debug(str(datetime.datetime.now()) + " WORKER - Khaled: process(task=" + str(task['id']) + ") running proces") 
    log.debug(str(datetime.datetime.now()) + " WORKER - Khaled: process(task=" + str(task['id']) + ") RunProcessing started") 
    results = RunProcessing(task=task).run()
    log.debug(str(datetime.datetime.now()) + " WORKER - Khaled: process(task=" + str(task['id']) + ") done RunProcessing, starting RunSignatures")
    RunSignatures(results=results).run()
    log.debug(str(datetime.datetime.now()) + " WORKER - Khaled: process(task=" + str(task['id']) + ") Done RunSignatures, moving into reporting")    

    if report:
        RunReporting(task=task, results=results).run()
        
	if auto:
            if cfg.cuckoo.delete_original and os.path.exists(target):
                os.unlink(target)

            if cfg.cuckoo.delete_bin_copy and copy_path and \
                    os.path.exists(copy_path):
                os.unlink(copy_path)
    log.debug(str(datetime.datetime.now()) + " WORKER - Khaled: process(task=" + str(task['id']) + ") task is DONE")
   


def createReportIfNeeded(target=None, copy_path=None, task=None, report=False, auto=False):

    #Since Dvasion relies on the existance of report.json, we need to make sure it exists
    report_path = os.path.join(CUCKOO_ROOT,"storage","analyses",str(task['id']),"reports","report.json")

    if not os.path.exists(os.path.dirname(report_path)):
    	try:
        	os.makedirs(os.path.dirname(report_path))
    	except OSError as exc: # Guard against race condition
		log.debug("Folder already exists")
 
    if (not os.path.isfile(report_path)):
    	with open(report_path,'w') as f:
		f.write(" ")

def process_wrapper(*args, **kwargs):
    try:
        process(*args, **kwargs)
    	#Since Dvasion relies on the existance of report.json, we need to make sure it exists
	createReportIfNeeded(*args,**kwargs)
    except Exception as e:
    	#Since Dvasion relies on the existance of report.json, we need to make sure it exists
	createReportIfNeeded(*args,**kwargs)
        e.traceback = traceback.format_exc()
        raise e

def init_worker():
    signal.signal(signal.SIGINT, signal.SIG_IGN)

def autoprocess(parallel=1):
    maxcount = cfg.cuckoo.max_analysis_count
    count = 0
    db = Database()
    pending_results = {}

    log.info("Calling autoprocess")

    # Respawn a worker process every 1000 tasks just in case we
    # have any memory leaks.
    pool = multiprocessing.Pool(processes=parallel, initializer=init_worker,
                                maxtasksperchild=1000)

    try:
        while True:
            # Pending results maintenance.
	    num_tasks_not_ready = 0
            for tid, ar in pending_results.items():
                if not ar.ready():
		    num_tasks_not_ready = num_tasks_not_ready + 1
                    continue

                log.debug(str(datetime.datetime.now()) + " SCHEDULER - Khaled: " + str(num_tasks_not_ready) + " tasks are not ready")

                if ar.successful():
                    log.info("Task #%d: reports generation completed", tid)
                    db.set_status(tid, TASK_REPORTED)
                    log.info("Task #%d: updated database", tid)
                else:
                    try:
                        log.debug(ar)                        
                        rr = ar.get()
                        
	                log.warning(str(datetime.datetime.now()) + " SCHEDULER - Khaled:  Task number: " + str(tid) + " is not ready with result: " + str(rr))
                    except Exception as e:
                        log.critical("Task #%d: exception in reports generation: %s", tid, e)
                        if hasattr(e, "traceback"):
                            log.info(e.traceback)
		    
                    log.info("Task #%d: updating database", tid)
                    db.set_status(tid, TASK_FAILED_PROCESSING)
                    log.info("Task #%d: updated database", tid)
		log.debug(str(datetime.datetime.now()) + " SCHEDULER - Khaled: removing task number: " + str(tid) + " from pending results")
                pending_results.pop(tid)
                count += 1

            # Make sure our queue has plenty of tasks in it.
            if len(pending_results) >= QUEUE_THRESHOLD:
		log.debug(str(datetime.datetime.now()) + " SCHEDULER - Khaled: length of pending results is large (" + str (len(pending_results)) + ") - sleeping for one second")
                time.sleep(1)
                continue

            # End of processing?
            if maxcount and count == maxcount:
                break

            # No need to submit further tasks for reporting as we've already
            # gotten to our maximum.
            if maxcount and count + len(pending_results) == maxcount:
		log.info("We reached our maximum tasks")
                time.sleep(1)
                continue

            # Get at most queue threshold new tasks. We skip the first N tasks
            # where N is the amount of entries in the pending results list.
            # Given we update a tasks status right before we pop it off the
            # pending results list it is guaranteed that we skip over all of
            # the pending tasks in the database and no further.
            if maxcount:
                limit = maxcount - count - len(pending_results)
            else:
                limit = QUEUE_THRESHOLD

            log.info("Getting list from database")
            tasks = db.list_tasks(status=TASK_COMPLETED,
                                  offset=len(pending_results),
                                  limit=min(limit, QUEUE_THRESHOLD),
                                  order_by=Task.completed_on)
	
	    log.debug(str(datetime.datetime.now()) + " SCHEDULER - Khaled:  fetched new " + str(len(tasks)) + " completed tasks from the database")

            # No new tasks, we can wait a small while before we query again
            # for new tasks.
            if not tasks:
		log.debug(str(datetime.datetime.now()) + " SCHEDULER - Khaled: no available tasks. Sleeping for 5 seconds")
                time.sleep(5)
                continue

            for task in tasks:
                log.debug(str(datetime.datetime.now()) + " SCHEDULER - Khaled: Handling task number: " + str(task.id))
		# Ensure that this task is not already in the pending list.
                # This is really mostly for debugging and should never happen.
                #assert task.id not in pending_results
		#Khaled: the above assertion is happening alot. Removing it
		if task.id in pending_results:
			log.debug(str(datetime.datetime.now()) + " SCHEDULER - Khaled: A task is already in the pending_results, should assert, but skipping it. Task: " + str(task.id))
			continue

                log.info(" SCHEDULER - Task #%d: queueing for reporting", task.id)

		#Kapil: Turning this functionality off since this leads to
		#hanging of process.py, just making copy_path as empty

                #if task.category == "file":
		#    log.info("Opening sample")
                #    sample = db.view_sample(task.sample_id)
		#    log.info("Opened sample")

                #    copy_path = os.path.join(CUCKOO_ROOT, "storage",
                #                             "binaries", sample.sha256)
                #else:
                #    copy_path = None
		copy_path = None
		
                args = task.target, copy_path
                kwargs = {
                    "report": True,
                    "auto": True,
                    "task": dict(task.to_dict()),
                }
                result = pool.apply_async(process_wrapper, args, kwargs)
		log.info(" Called Results on Task %d",task.id)
		pending_results[task.id] = result
		log.debug(str(datetime.datetime.now()) + " SCHEDULER - Khaled: added task: " + str(task.id) + " to pending_results")
    except KeyboardInterrupt:
        pool.terminate()
        raise
    except:
        log.exception("Caught unknown exception")
    finally:
        pool.join()

def main():
    global log
	
    listen()

    parser = argparse.ArgumentParser()
    parser.add_argument("id", type=str, help="ID of the analysis to process (auto for continuous processing of unprocessed tasks).")
    parser.add_argument("-d", "--debug", help="Display debug messages", action="store_true", required=False)
    parser.add_argument("-r", "--report", help="Re-generate report", action="store_true", required=False)
    parser.add_argument("-p", "--parallel", help="Number of parallel threads to use (auto mode only).", type=int, required=False, default=1)
    parser.add_argument("-u", "--user", type=str, help="Drop user privileges to this user")
    parser.add_argument("-m", "--modules", help="Path to signature and reporting modules - overrides default modules path.", type=str, required=False)

    args = parser.parse_args()

    if args.user:
        drop_privileges(args.user)

    if args.debug:
        logging.basicConfig(level=logging.DEBUG)
    else:
        logging.basicConfig(level=logging.INFO)

    log = logging.getLogger("cuckoo.process")

    if args.modules:
        sys.path.insert(0, args.modules)

    init_modules(machinery=False)

    if args.id == "auto":
        autoprocess(parallel=args.parallel)
    else:
        task = Database().view_task(int(args.id))
        if not task:
            process(task={"id": int(args.id), "category": "file", "target": ""}, report=args.report)
        else:
            process(task=task.to_dict(), report=args.report)

if __name__ == "__main__":
    cfg = Config()

    try:
        main()
    except KeyboardInterrupt:
        pass
