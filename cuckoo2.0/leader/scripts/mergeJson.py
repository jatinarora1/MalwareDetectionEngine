#!/usr/bin/env python

import os
import argparse
import json
import codecs
from collections import OrderedDict
import shutil
import time
import sys
import logging
import tempfile
import truncate_json
from MalwareSampleReport import MalwareSampleReport

MAX_JSON_READ_ATTEMPTS = 40
POLL_JSON_SLEEP = 5

ORIG_CUCKOO_REPORT_NAME = "origCuckoo.json"
DVASION_CUCKOO_UNTRUNCATED_REPORT_NAME = "dvasionCuckoo-untrunc.json"
DVASION_CUCKOO_FINAL_REPORT_NAME = "dvasionCuckoo.json"

MLTOOL_BASE_CMD = "./scripts/MalwareML/simple_ml.py -m " 
MLTOOL_MODEL_PATH = " scripts/MLModel/ "


# Signatures to skip
skip_sigs = ["Virustotal as malicious"]


# Dropped files to skip, results of our analyses
skip_sw_files = ["EMP_Response.bin","Lib.txt","EMP_Debug.out","EMP_Request.bin","ProcessInterceptorDebug.txt","LWR_outDumpBin.txt","CFG.txt","CallGraph.txt","EMP_ProgFeatures.bin"]


#logging.basicConfig(format='%(asctime)s - %(levelname)s: %(message)s',filename='mergeJson.log',level=logging.DEBUG)

# Taken from cuckoo's json dump
def default(obj):
    if isinstance(obj, datetime.datetime):
        if obj.utcoffset() is not None:
            obj = obj - obj.utcoffset()
        return calendar.timegm(obj.timetuple()) + obj.microsecond / 1000.0
    raise TypeError("%r is not JSON serializable" % obj)


def pruneFiles(origList, checkName):
	#Input must be dropped files data

	#Go through all the dropped files and if any of them matches our 
	#list, then add them to the list
	index=0
	indicesToDelete = []
	for curfile in origList:
		fileName = curfile
		#Check if we should lookup name field
		if checkName==True: 
			fileName = curfile["name"] 
		
		# We need to ignore extensions or file paths
		for tempFile in skip_sw_files:
			if tempFile in fileName:
				print "Deleting " + fileName
				
				indicesToDelete.append(index)
		index=index+1
	
	#Now go through the list and delete the indices
	for index in sorted(indicesToDelete,reverse=True):
		del origList[index]
	
	

# General merging code, Merge_Type is the main section heading, while
# comp_field is the field to further compare entries (if this is applicable for a particular type)
def MergeChild(origData,newData,merge_type,comp_field):
	
	# If the field isn't in the original data, we will just add this entire section
	# Return this section
	if (not merge_type in origData):
		try:
			return newData[merge_type]
		except KeyError as e:
			return []

	sig_list = origData[merge_type]

	if (comp_field):
		# Get a list of just the values in the comparison field (the "descriptions")
		# This is done through list comprehension
		sig_descs = [desc[comp_field] for desc in sig_list]
		
		# Loop through the values in the new data. If they are not in the old data,
		# add them to the return list. If there is some issue with the "new" data (missing keys), we don't
		# want the entire script to fail. Just skip these but add something to the log.
		try:
			for sig in newData[merge_type]:
				if (sig[comp_field] not in sig_descs) and (sig[comp_field] not in skip_sigs):
					sig_list.append(sig)
		except KeyError as e:
			pass
		except Exception as e:
			logging.error("Encountered exception %s" % e)
	else:
		try:
			for sig in newData[merge_type]:
				if not isinstance(sig_list,list):
					continue
				if sig not in sig_list:
					sig_list.append(sig)
		except KeyError as e:
			pass
		except Exception as e:
			logging.error("Encountered exception %s" % e)

	return sig_list

# Merge "Summary" section of behavior
def MergeBehavior(origData,newData):
	# Make a copy of the summary dict
	try:
		origSummary = origData["behavior"]["summary"]
	except KeyError as e:
		origSummary = {}
	
	try:
		newSummary = newData["behavior"]["summary"]
	except KeyError as e:
		return origSummary
	
	# Loop through all of the keys in the summary dict within "new data"
	for sumKey in newSummary:
		if origSummary == None:
			continue
		# Since the summary section is a dict as well, we can reuse the MergeChild code
		# Just compare data a level lower than before
		origSummary[sumKey] = MergeChild(origSummary,newSummary,sumKey,None)

	return origSummary

def MergeNetwork(origData,newData):
	# Everything in the network results are lists, except for these:
	merge_dicts = ["country_percents","countries"]

	# Merge list data normally...
	# Make a copy of the network dict
	try:
		origNetwork = origData["network"]
	except KeyError as e:
		origNetwork = {}
	
	try:
		newNetwork = newData["network"]
	except KeyError as e:
		return origNetwork
	
	# Loop through all of the keys in the network
	for sumKey in newNetwork:
		if origNetwork == None:
			continue
		# If the key is one of the dicts, skip it
		if sumKey in merge_dicts:
			continue
		origNetwork[sumKey] = MergeChild(origNetwork,newNetwork,sumKey,None)
	
	try:
		if (not origNetwork == None):
			origNetwork["countries"] = MergeChild(origNetwork,newNetwork,"countries","ip")
	except KeyError as e:
		pass


	# Skip merging country percents. This data will need to be calculated at the end

	return origNetwork


def PruneExtraFiles(origJsonData):
	
	#We remove our internal files from this list
	try:
		print "Pruning Dropped Files" 
		pruneFiles(origJsonData["dropped"],True)
	except:		
		print "Failed to remove some of our files"

	try:
		print "Pruning File Read" 
		pruneFiles(origJsonData["behavior"]["summary"]["file_read"],False)
	except:
		print "Failed to remove some of our files"

	try:
		print "Pruning File Opened" 
		pruneFiles(origJsonData["behavior"]["summary"]["file_opened"],False)
	except:
		print "Failed to remove some of our files"
	
	try:
		print "Pruning File Exists" 
		pruneFiles(origJsonData["behavior"]["summary"]["file_exists"],False)
	except:
		print "Failed to remove some of our files"


	try:
		print "Pruning File Written" 
		pruneFiles(origJsonData["behavior"]["summary"]["file_written"],False)
	except:
		print "Failed to remove some of our files"

	
	return origJsonData


def MergeCurrentJSONData(origJsonData,newData):
	origJsonData["signatures"] = MergeChild(origJsonData,newData,"signatures","description")
	origJsonData["dropped"] = MergeChild(origJsonData,newData,"dropped","name")
	origJsonData["strings"] = MergeChild(origJsonData,newData,"strings",None)
	origJsonData["behavior"]["summary"] = MergeBehavior(origJsonData,newData)
	origJsonData["network"] = MergeNetwork(origJsonData,newData)

	origJsonData = PruneExtraFiles(origJsonData)

	return origJsonData

def LoadJsonSafely(fileName,objectHook):
	numTrials=0
	print "Loading file: " + fileName
	while(numTrials < MAX_JSON_READ_ATTEMPTS):
		try:
			if objectHook==True:
				with open(fileName) as opened_file:
					jsonData = json.load(opened_file,object_pairs_hook=OrderedDict)
					print "Loaded file"
					return jsonData
			else:
				with open(fileName) as opened_file:
					jsonData = json.load(opened_file)
					print "Loaded file"
					return jsonData
		except:
			numTrials=numTrials+1
			logging.error("Failed to read file %s. Trying again",fileName)
			time.sleep(POLL_JSON_SLEEP)
				
	logging.error("Failed to read file %s after all attempts. Exiting",fileName)
	sys.exit(0)

def LoadAndMergeData(origJsonData,inFile, init_data):

	if (not init_data):
		origJsonData = LoadJsonSafely(inFile,True)
		origJsonData = PruneExtraFiles(origJsonData)
	else:
		newData = LoadJsonSafely(inFile,False)
		origJsonData = MergeCurrentJSONData(origJsonData,newData)

	return origJsonData
	


def merge_files(orig_files, new_files,outputDirName,newScore):
	outFilename = os.path.join(outputDirName, "MergeJson")
	outMergeName = os.path.join(outputDirName,DVASION_CUCKOO_UNTRUNCATED_REPORT_NAME)

	if os.path.isfile(outMergeName):
		os.remove(outMergeName)

	init_data = False
	origJsonData = None

	for new_file in new_files:
		
		print "Merging File " + new_file	
		origJsonData = LoadAndMergeData(origJsonData,new_file,init_data)
		init_data=True

	for orig_file in orig_files:
		
		print "Merging Original File " + orig_file

		origJsonData = LoadAndMergeData(origJsonData,orig_file,init_data)
		init_data=True
		
	
	#Now, add the signature classification field
	
	


	#Update score in data
	try:
		if newScore > 10:
			newScore = 10
		origJsonData["info"]["score"] = newScore
	except:
		logging.error("Failed to update score, field not found")

	print "Updating network info"
	if "network" in origJsonData and "countries" in origJsonData["network"]:
		# Network country percents need to be recalculated properly
		num_countries = len(origJsonData["network"]["countries"])

		country_totals = {}
		country_percents= {}
		for connection in origJsonData["network"]["countries"]:
			country = connection["country"]
			if (country != "null"):
				try:
					country_totals[country] = country_totals[country] + 1
				except KeyError:
					country_totals[country] = 1
		for c in country_totals:
			country_percents[c] = "{0:.0f}".format((country_totals[c]/float(num_countries)) * 100)

		origJsonData["network"]["country_percents"] = country_percents	

		origJsonData["network"]["countries"] = sorted(origJsonData["network"]["countries"],key=lambda k: k['country'])

	try:
		with codecs.open(outMergeName, "w", "utf-8") as report:
			json.dump(origJsonData, report, default=default,  sort_keys=False, indent=4,
			encoding="utf-8")
	except (UnicodeError, TypeError, IOError) as e:
	    logging.error("Failed to generate JSON report: %s" % e)
	    sys.exit(0)



def update_score_from_machine_learning(filename):
	tf = tempfile.NamedTemporaryFile(delete=False)	

	#Form ML command which will dump the output to a temporary file
	mlCmd = MLTOOL_BASE_CMD + MLTOOL_MODEL_PATH + " -ev " + filename + " > " + tf.name 
	
	#Run the command
	os.system(mlCmd)

	#Parse output
	lines = [line.rstrip('\n') for line in open(tf.name)]

	#Remove temporary file
	os.remove(tf.name)

	maliciousStr=""
	for line in lines:
		if line == "0":
			maliciousStr = "This sample is benign!"
			break
		if line == "1":
			maliciousStr = "This sample is malicious!"
			break

	#IF we do not get any output from tool, dont modify the file
	if maliciousStr=="":
		return

	reportData = MalwareSampleReport(filename)
	reportData.set_malware_result(maliciousStr)
	reportData.dump_json_file(filename)
	

if __name__ == "__main__":
	p=argparse.ArgumentParser()
	
	p.add_argument("-s,", "--score",type=float, help="New cuckoo score",required=True)
	p.add_argument("-o,", "--orig",type=str, help="An original report",nargs='*')
	p.add_argument("-n", "--new",type=str, help="A dvasion run report",nargs='+')
	p.add_argument("-f","--file",type=str,help="The output directory for the merged json file",required=True)
	p.add_argument("-m","--runml",action="store_true",default="False",help="Run ML to compute result")
	args = p.parse_args()

	runML = False
	if args.runml==True:
		runML = True
		print "Running ML"
	else:
		print "Not running ML"	

	newScore = args.score
	new_files = args.new
	if new_files == None:
		new_files = []
	orig_files = args.orig
	if orig_files == None:
		orig_files = []
	outputDirName=args.file

	outLogFile = outputDirName + "/mergeJson.log"
	logging.basicConfig(format='%(asctime)s - %(levelname)s: %(message)s',filename=outLogFile,level=logging.DEBUG)

	outMergeName = os.path.join(outputDirName,DVASION_CUCKOO_UNTRUNCATED_REPORT_NAME)
	outFinalName = os.path.join(outputDirName,DVASION_CUCKOO_FINAL_REPORT_NAME)

	merge_files(orig_files, new_files,outputDirName,newScore)
	
	print "Merged Files. Truncating  file "	 + outMergeName
	
	#After merging, we truncate the files
	
	try:
		truncate_json.truncate_json(outMergeName,outFinalName)
	except Exception as e:
		print  "Failed to truncate JSON Report: %s: Copying untruncated file " + outMergeName
		logging.exception("Failed to truncate JSON Report: %s: Copying untruncated file ",outMergeName)
	        shutil.copy2(outMergeName,outFinalName)	

	if runML==True:
		print "Getting decision through ML"
		#Run MalwareML to get a decision based on machine learning module
		update_score_from_machine_learning(outFinalName)	

	#Delete merged untruncated file	
	os.remove(outMergeName)
